import os.path
from numpy.random import randint
from torch.utils import data
import glob
import os
from dataloader.video_transform import *
import numpy as np
import librosa
import torchaudio
from timm.models.layers import to_2tuple


class VideoRecord(object):
    def __init__(self, row):
        self._data = row

    @property
    def path(self):
        return self._data[0]

    @property
    def num_frames(self):
        return int(self._data[1])

    def set_num_frames(self, nn):
        self._data[1] = nn

    @property
    def label(self):
        return float(self._data[2])


class VideoDataset(data.Dataset):
    def __init__(self, list_file, num_segments, duration, mode, transform, image_size):

        self.list_file = list_file
        # 帧图片集
        self.duration = duration
        # 每个片段截取几张
        self.num_segments = num_segments
        # 所有图片截取多少张图片
        self.transform = transform
        self.image_size = image_size
        # 图片尺寸
        self.mode = mode
        # 训练或者测试
        self._parse_list()
        pass

    def _parse_list(self):
        #
        # Data Form: [video_id, num_frames, class_idx]
        #
        tmp = [x.strip().split(' ') for x in open(self.list_file)]
        tmp = [item for item in tmp]
        self.video_list = [VideoRecord(item) for item in tmp]
        print(('video number:%d' % (len(self.video_list))))

    def _get_train_indices(self, record):
        #
        # Split all frames into seg parts, then select frame in each part randomly
        #
        average_duration = (record.num_frames - self.duration + 1) // self.num_segments
        if average_duration > 0:
            offsets = np.multiply(list(range(self.num_segments)), average_duration) + randint(average_duration,
                                                                                              size=self.num_segments)
        elif record.num_frames > self.num_segments:
            offsets = np.sort(randint(record.num_frames - self.duration + 1, size=self.num_segments))
        else:
            offsets = np.pad(np.array(list(range(record.num_frames))), (0, self.num_segments - record.num_frames),
                             'edge')
        return offsets

    def _get_test_indices(self, record):
        #
        # Split all frames into seg parts, then select frame in the mid of each part
        #
        if record.num_frames > self.num_segments + self.duration - 1:
            tick = (record.num_frames - self.duration + 1) / float(self.num_segments)
            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])
        else:
            offsets = np.pad(np.array(list(range(record.num_frames))), (0, self.num_segments - record.num_frames),
                             'edge')
        return offsets

    def __getitem__(self, index, segment_indices=None):
        record = self.video_list[index]
        if self.mode == 'train':
            segment_indices = self._get_train_indices(record)
            return self.get(record, segment_indices)

        elif self.mode == 'test':
            segment_indices = self._get_test_indices(record)
            return self.get(record, segment_indices)

        return self.get(record, segment_indices)

    def _wav2fbank(self, filename, filename2=None):
        if filename2 == None:
            waveform, sr = torchaudio.load(filename)
            waveform = waveform - waveform.mean()
        # mixup
        else:
            waveform1, sr = torchaudio.load(filename)
            waveform2, _ = torchaudio.load(filename2)

            waveform1 = waveform1 - waveform1.mean()
            waveform2 = waveform2 - waveform2.mean()

            if waveform1.shape[1] != waveform2.shape[1]:
                if waveform1.shape[1] > waveform2.shape[1]:
                    # padding
                    temp_wav = torch.zeros(1, waveform1.shape[1])
                    temp_wav[0, 0:waveform2.shape[1]] = waveform2
                    waveform2 = temp_wav
                else:
                    # cutting
                    waveform2 = waveform2[0, 0:waveform1.shape[1]]

            # sample lambda from beta distribtion
            mix_lambda = np.random.beta(10, 10)

            mix_waveform = mix_lambda * waveform1 + (1 - mix_lambda) * waveform2
            waveform = mix_waveform - mix_waveform.mean()
        # 498 128, 998, 128
        fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sr, use_energy=False,
                                                  window_type='hanning', num_mel_bins=128, dither=0.0, frame_shift=10)
        # 12
        target_length = 512
        n_frames = fbank.shape[0]

        p = target_length - n_frames

        # cut and pad
        if p > 0:
            m = torch.nn.ZeroPad2d((0, 0, 0, p))
            fbank = m(fbank)
        elif p < 0:
            fbank = fbank[0:target_length, :]

        if filename2 == None:
            return fbank, 0
        else:
            return fbank, mix_lambda

    def get(self, record, indices):

        video_frames_path = glob.glob(os.path.join(record.path, '*'))
        video_frames_path.sort()
        # 在video_frames_path.sort()后添加
        actual_num_frames = len(video_frames_path)
        if actual_num_frames != record.num_frames:
            print(f"警告：视频 {record.path} 记录帧数={record.num_frames}，实际帧数={actual_num_frames}")
        images = list()
        for seg_ind in indices:
            p = int(seg_ind)
            for i in range(self.duration):
                seg_imgs = [Image.open(os.path.join(video_frames_path[p])).convert('RGB')]
                images.extend(seg_imgs)
                if p < record.num_frames - 1:
                    p += 1

        video_folder_name = video_frames_path[p].split('/')[-2]  # 得到"00001"
        if self.mode == 'train':
            audio_path = '/root/autodl-tmp/AVEC/wav/train/' + video_folder_name + '.wav'
        elif self.mode == 'test':
            audio_path = '/root/autodl-tmp/AVEC/wav/test/' + video_folder_name + '.wav'
        # 4. 加载音频
        fbank, mix_lambda = self._wav2fbank(audio_path)

        freqm = torchaudio.transforms.FrequencyMasking(0)
        timem = torchaudio.transforms.TimeMasking(0)
        fbank = fbank.transpose(0, 1).unsqueeze(0)
        fbank = torch.transpose(fbank.squeeze(), 0, 1)  # time, freq
        fbank = (fbank - (-4.2677393)) / (4.5689974 * 2)  # audioset

        images = self.transform(images)
        images = torch.reshape(images, (-1, 3, self.image_size, self.image_size))

        return images, record.label, fbank.unsqueeze(0)

    def __len__(self):
        return len(self.video_list)


def train_data_loader(list_file, num_segments, duration, image_size, args):
    train_transforms = torchvision.transforms.Compose([
        ColorJitter(brightness=0.5),
        GroupRandomSizedCrop(image_size),
        GroupRandomHorizontalFlip(),
        Stack(),
        ToTorchFormatTensor()])

    train_data = VideoDataset(list_file=list_file,
                              num_segments=num_segments,
                              duration=duration,
                              mode='train',
                              transform=train_transforms,
                              image_size=image_size)
    return train_data


def test_data_loader(list_file, num_segments, duration, image_size):
    test_transform = torchvision.transforms.Compose([GroupResize(image_size),
                                                     Stack(),
                                                     ToTorchFormatTensor()])

    test_data = VideoDataset(list_file=list_file,
                             num_segments=num_segments,
                             duration=duration,
                             mode='test',
                             transform=test_transform,
                             image_size=image_size)
    return test_data
